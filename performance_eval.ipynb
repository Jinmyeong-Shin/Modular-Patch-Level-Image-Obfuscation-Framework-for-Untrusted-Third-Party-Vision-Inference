{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbffcfb9",
   "metadata": {},
   "source": [
    "# Performancce Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cfd65a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1909031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "\n",
    "# datasets.load_dataset('benjamin-paine/imagenet-1k-256x256')\n",
    "# datasets.load_dataset('AISNP/COCO2017-instances')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f427b60e",
   "metadata": {},
   "source": [
    "## Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a495b8",
   "metadata": {},
   "source": [
    "### google/vit-base-patch16-224-in21k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45746e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'google/vit-base-patch16-224-in21k'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2910ddd",
   "metadata": {},
   "source": [
    "#### CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc96654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from data.model_config import ImageClassificationModelConfigWithObfuscation\n",
    "from model.classification import ImageClassificationModel\n",
    "\n",
    "model_config = ImageClassificationModelConfigWithObfuscation(\n",
    "    hf_model_name_or_path=MODEL_NAME,\n",
    "    num_classes=10,\n",
    "    obfuscation_patch_size=14,\n",
    "    obfuscation_group_size=128\n",
    ")\n",
    "\n",
    "model = ImageClassificationModel(config=model_config, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7644fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageClassificationModel.train_obfuscation_modules(model, iterations=500, learning_rate=1e-2, batch_size=128)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defec4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset_config import load_dataset_config\n",
    "\n",
    "dataset_config = load_dataset_config('cifar10')\n",
    "\n",
    "_, eval_dataset = dataset_config.build()\n",
    "label_names = [name for cls, name in dataset_config.id2label.items()]\n",
    "\n",
    "samples = eval_dataset.shuffle().select(range(5))\n",
    "\n",
    "obfuscated_samples = model.obfuscate(list(samples[dataset_config.input_column])).obfuscated_images.cpu()\n",
    "obfuscated_images = (obfuscated_samples + 1) / 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(2, len(samples), figsize=(20, 10))\n",
    "\n",
    "for i, (sample, obfuscated_sample) in enumerate(zip(samples, obfuscated_images)):\n",
    "    sample_name = sample[dataset_config.label_column]\n",
    "    sample_image = sample[dataset_config.input_column]\n",
    "\n",
    "    axes[0, i].set_title(f'{sample_name}')\n",
    "    axes[0, i].imshow(sample_image.permute(1, 2, 0))\n",
    "    axes[1, i].imshow(obfuscated_sample.permute(1, 2, 0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageClassificationModel.train_model(model, dataset_config, iterations=1000, learning_rate=1e-4, batch_size=256, mixed_precision='fp16')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d7b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_wo_obfuscation = ImageClassificationModel.evaluate_model(model, dataset_config, batch_size=256, with_obfuscation=False)\n",
    "print(f'Evaluation Rsults on Clean Images: accuracy - {eval_result_wo_obfuscation.accuracy}, recall = {eval_result_wo_obfuscation.recall}, precision = {eval_result_wo_obfuscation.precision}, f1 - {eval_result_wo_obfuscation.f1}')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2dc8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_w_obfuscation = ImageClassificationModel.evaluate_model(model, dataset_config, batch_size=256, with_obfuscation=True)\n",
    "print(f'Evaluation Rsults on Obfuscated Images: accuracy - {eval_result_w_obfuscation.accuracy}, recall = {eval_result_w_obfuscation.recall}, precision = {eval_result_w_obfuscation.precision}, f1 - {eval_result_w_obfuscation.f1}')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac5018",
   "metadata": {},
   "source": [
    "#### CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3805b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from data.model_config import ImageClassificationModelConfigWithObfuscation\n",
    "from model.classification import ImageClassificationModel\n",
    "\n",
    "model_config = ImageClassificationModelConfigWithObfuscation(\n",
    "    hf_model_name_or_path=MODEL_NAME,\n",
    "    num_classes=100,\n",
    "    obfuscation_patch_size=14,\n",
    "    obfuscation_group_size=128\n",
    ")\n",
    "\n",
    "model = ImageClassificationModel(config=model_config, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5661b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageClassificationModel.train_obfuscation_modules(model, iterations=500, learning_rate=1e-2, batch_size=128)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1695e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset_config import load_dataset_config\n",
    "\n",
    "dataset_config = load_dataset_config('cifar10')\n",
    "\n",
    "_, eval_dataset = dataset_config.build()\n",
    "label_names = [name for cls, name in dataset_config.id2label.items()]\n",
    "\n",
    "samples = eval_dataset.shuffle().select(range(5))\n",
    "\n",
    "obfuscated_samples = model.obfuscate(list(samples[dataset_config.input_column])).obfuscated_images.cpu()\n",
    "obfuscated_images = (obfuscated_samples + 1) / 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(2, len(samples), figsize=(20, 10))\n",
    "\n",
    "for i, (sample, obfuscated_sample) in enumerate(zip(samples, obfuscated_images)):\n",
    "    sample_name = sample[dataset_config.label_column]\n",
    "    sample_image = sample[dataset_config.input_column]\n",
    "\n",
    "    axes[0, i].set_title(f'{sample_name}')\n",
    "    axes[0, i].imshow(sample_image.permute(1, 2, 0))\n",
    "    axes[1, i].imshow(obfuscated_sample.permute(1, 2, 0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageClassificationModel.train_model(model, dataset_config, iterations=1000, learning_rate=1e-4, batch_size=512)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f57099",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_wo_obfuscation = ImageClassificationModel.evaluate_model(model, dataset_config, batch_size=256, with_obfuscation=False)\n",
    "print(f'Evaluation Rsults on Clean Images: accuracy - {eval_result_wo_obfuscation.accuracy}, recall = {eval_result_wo_obfuscation.recall}, precision = {eval_result_wo_obfuscation.precision}, f1 - {eval_result_wo_obfuscation.f1}')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c3c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_w_obfuscation = ImageClassificationModel.evaluate_model(model, dataset_config, batch_size=256, with_obfuscation=True)\n",
    "print(f'Evaluation Rsults on Obfuscated Images: accuracy - {eval_result_w_obfuscation.accuracy}, recall = {eval_result_w_obfuscation.recall}, precision = {eval_result_w_obfuscation.precision}, f1 - {eval_result_w_obfuscation.f1}')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b1bde3",
   "metadata": {},
   "source": [
    "#### PathMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e329bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from data.model_config import ImageClassificationModelConfigWithObfuscation\n",
    "from model.classification import ImageClassificationModel\n",
    "\n",
    "model_config = ImageClassificationModelConfigWithObfuscation(\n",
    "    hf_model_name_or_path=MODEL_NAME,\n",
    "    num_classes=9,\n",
    "    obfuscation_patch_size=14,\n",
    "    obfuscation_group_size=128\n",
    ")\n",
    "\n",
    "model = ImageClassificationModel(config=model_config, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ccb9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageClassificationModel.train_obfuscation_modules(model, iterations=1000, learning_rate=1e-2, batch_size=128)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbed56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset_config import load_dataset_config\n",
    "\n",
    "dataset_config = load_dataset_config('pathmnist')\n",
    "\n",
    "_, eval_dataset = dataset_config.build()\n",
    "label_names = [name for cls, name in dataset_config.id2label.items()]\n",
    "\n",
    "samples = eval_dataset.shuffle().select(range(5))\n",
    "\n",
    "obfuscated_samples = model.obfuscate(list(samples[dataset_config.input_column])).obfuscated_images.cpu()\n",
    "obfuscated_images = (obfuscated_samples + 1) / 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(2, len(samples), figsize=(20, 10))\n",
    "\n",
    "for i, (sample, obfuscated_sample) in enumerate(zip(samples, obfuscated_images)):\n",
    "    sample_name = sample[dataset_config.label_column]\n",
    "    sample_image = sample[dataset_config.input_column]\n",
    "\n",
    "    axes[0, i].set_title(f'{sample_name}')\n",
    "    axes[0, i].imshow(sample_image.permute(1, 2, 0))\n",
    "    axes[1, i].imshow(obfuscated_sample.permute(1, 2, 0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageClassificationModel.train_model(model, dataset_config, iterations=100, learning_rate=1e-4, batch_size=256)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_wo_obfuscation = ImageClassificationModel.evaluate_model(model, dataset_config, batch_size=256, with_obfuscation=False)\n",
    "print(f'Evaluation Rsults on Clean Images: accuracy - {eval_result_wo_obfuscation.accuracy}, recall = {eval_result_wo_obfuscation.recall}, precision = {eval_result_wo_obfuscation.precision}, f1 - {eval_result_wo_obfuscation.f1}')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee7e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_w_obfuscation = ImageClassificationModel.evaluate_model(model, dataset_config, batch_size=256, with_obfuscation=True)\n",
    "print(f'Evaluation Rsults on Obfuscated Images: accuracy - {eval_result_w_obfuscation.accuracy}, recall = {eval_result_w_obfuscation.recall}, precision = {eval_result_w_obfuscation.precision}, f1 - {eval_result_w_obfuscation.f1}')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcebb61b",
   "metadata": {},
   "source": [
    "## Multi-Modal Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090644c8",
   "metadata": {},
   "source": [
    "### openai/clip-vit-base-patch32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3074018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.model_config import ImageModelConfigWithObfuscation\n",
    "from model.classification import ZeroShotImageClassificationModel\n",
    "\n",
    "MODEL_NAME = 'openai/clip-vit-base-patch32'\n",
    "\n",
    "model_config = ImageModelConfigWithObfuscation(\n",
    "    hf_model_name_or_path=MODEL_NAME,\n",
    "    obfuscation_patch_size=14,\n",
    "    obfuscation_group_size=128\n",
    ")\n",
    "\n",
    "model = ZeroShotImageClassificationModel(config=model_config, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3190219",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ZeroShotImageClassificationModel.train_obfuscation_modules(model, iterations=500, learning_rate=1e-2, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2621794a",
   "metadata": {},
   "source": [
    "#### CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6598b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset_config import load_dataset_config\n",
    "\n",
    "dataset_config = load_dataset_config('cifar10')\n",
    "\n",
    "_, eval_dataset = dataset_config.build()\n",
    "label_names = [name for cls, name in dataset_config.id2label.items()]\n",
    "\n",
    "samples = eval_dataset.shuffle().select(range(5))\n",
    "\n",
    "obfuscated_samples = model.obfuscate(list(samples[dataset_config.input_column])).obfuscated_images.cpu()\n",
    "obfuscated_images = (obfuscated_samples + 1) / 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(2, len(samples), figsize=(20, 10))\n",
    "\n",
    "for i, (sample, obfuscated_sample) in enumerate(zip(samples, obfuscated_images)):\n",
    "    sample_name = sample[dataset_config.label_column]\n",
    "    sample_image = sample[dataset_config.input_column]\n",
    "\n",
    "    axes[0, i].set_title(f'{sample_name}')\n",
    "    axes[0, i].imshow(sample_image.permute(1, 2, 0))\n",
    "    axes[1, i].imshow(obfuscated_sample.permute(1, 2, 0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_wo_obfuscation = ZeroShotImageClassificationModel.evaluate_model(model, dataset_config, batch_size=256, with_obfuscation=False)\n",
    "print(f'Evaluation Rsults on Clean Images: accuracy - {eval_result_wo_obfuscation.accuracy}, recall = {eval_result_wo_obfuscation.recall}, precision = {eval_result_wo_obfuscation.precision}, f1 - {eval_result_wo_obfuscation.f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_w_obfuscation = ZeroShotImageClassificationModel.evaluate_model(model, dataset_config, batch_size=256, with_obfuscation=True)\n",
    "print(f'Evaluation Rsults on Clean Images: accuracy - {eval_result_w_obfuscation.accuracy}, recall = {eval_result_w_obfuscation.recall}, precision = {eval_result_w_obfuscation.precision}, f1 - {eval_result_w_obfuscation.f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98994d67",
   "metadata": {},
   "source": [
    "#### CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd14e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset_config import load_dataset_config\n",
    "\n",
    "dataset_config = load_dataset_config('cifar100')\n",
    "\n",
    "_, eval_dataset = dataset_config.build()\n",
    "label_names = [name for cls, name in dataset_config.id2label.items()]\n",
    "\n",
    "samples = eval_dataset.shuffle().select(range(5))\n",
    "\n",
    "obfuscated_samples = model.obfuscate(list(samples[dataset_config.input_column])).obfuscated_images.cpu()\n",
    "obfuscated_images = (obfuscated_samples + 1) / 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(2, len(samples), figsize=(20, 10))\n",
    "\n",
    "for i, (sample, obfuscated_sample) in enumerate(zip(samples, obfuscated_images)):\n",
    "    sample_name = sample[dataset_config.label_column]\n",
    "    sample_image = sample[dataset_config.input_column]\n",
    "\n",
    "    axes[0, i].set_title(f'{sample_name}')\n",
    "    axes[0, i].imshow(sample_image.permute(1, 2, 0))\n",
    "    axes[1, i].imshow(obfuscated_sample.permute(1, 2, 0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44bdb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_wo_obfuscation = ZeroShotImageClassificationModel.evaluate_model(model, dataset_config, batch_size=256, with_obfuscation=False)\n",
    "print(f'Evaluation Rsults on Clean Images: accuracy - {eval_result_wo_obfuscation.accuracy}, recall = {eval_result_wo_obfuscation.recall}, precision = {eval_result_wo_obfuscation.precision}, f1 - {eval_result_wo_obfuscation.f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c09fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_w_obfuscation = ZeroShotImageClassificationModel.evaluate_model(model, dataset_config, batch_size=256, with_obfuscation=True)\n",
    "print(f'Evaluation Rsults on Clean Images: accuracy - {eval_result_w_obfuscation.accuracy}, recall = {eval_result_w_obfuscation.recall}, precision = {eval_result_w_obfuscation.precision}, f1 - {eval_result_w_obfuscation.f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd26b56",
   "metadata": {},
   "source": [
    "### openai/clip-vit-base-patch16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.model_config import ImageModelConfigWithObfuscation\n",
    "from model.classification import ZeroShotImageClassificationModel\n",
    "\n",
    "MODEL_NAME = 'openai/clip-vit-base-patch16'\n",
    "\n",
    "model_config = ImageModelConfigWithObfuscation(\n",
    "    hf_model_name_or_path=MODEL_NAME,\n",
    "    obfuscation_patch_size=14,\n",
    "    obfuscation_group_size=128\n",
    ")\n",
    "\n",
    "model = ZeroShotImageClassificationModel(config=model_config, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ZeroShotImageClassificationModel.train_obfuscation_modules(model, iterations=500, learning_rate=1e-2, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96209d84",
   "metadata": {},
   "source": [
    "#### CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece60383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset_config import load_dataset_config\n",
    "\n",
    "dataset_config = load_dataset_config('cifar10')\n",
    "\n",
    "_, eval_dataset = dataset_config.build()\n",
    "label_names = [name for cls, name in dataset_config.id2label.items()]\n",
    "\n",
    "samples = eval_dataset.shuffle().select(range(5))\n",
    "\n",
    "obfuscated_samples = model.obfuscate(list(samples[dataset_config.input_column])).obfuscated_images.cpu()\n",
    "obfuscated_images = (obfuscated_samples + 1) / 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(2, len(samples), figsize=(20, 10))\n",
    "\n",
    "for i, (sample, obfuscated_sample) in enumerate(zip(samples, obfuscated_images)):\n",
    "    sample_name = sample[dataset_config.label_column]\n",
    "    sample_image = sample[dataset_config.input_column]\n",
    "\n",
    "    axes[0, i].set_title(f'{sample_name}')\n",
    "    axes[0, i].imshow(sample_image.permute(1, 2, 0))\n",
    "    axes[1, i].imshow(obfuscated_sample.permute(1, 2, 0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54239366",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_wo_obfuscation = ZeroShotImageClassificationModel.evaluate_model(model, dataset_config, batch_size=256, with_obfuscation=False)\n",
    "print(f'Evaluation Rsults on Clean Images: accuracy - {eval_result_wo_obfuscation.accuracy}, recall = {eval_result_wo_obfuscation.recall}, precision = {eval_result_wo_obfuscation.precision}, f1 - {eval_result_wo_obfuscation.f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87855635",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_w_obfuscation = ZeroShotImageClassificationModel.evaluate_model(model, dataset_config, batch_size=256, with_obfuscation=True)\n",
    "print(f'Evaluation Rsults on Clean Images: accuracy - {eval_result_w_obfuscation.accuracy}, recall = {eval_result_w_obfuscation.recall}, precision = {eval_result_w_obfuscation.precision}, f1 - {eval_result_w_obfuscation.f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaebd06",
   "metadata": {},
   "source": [
    "#### CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc85e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset_config import load_dataset_config\n",
    "\n",
    "dataset_config = load_dataset_config('cifar100')\n",
    "\n",
    "_, eval_dataset = dataset_config.build()\n",
    "label_names = [name for cls, name in dataset_config.id2label.items()]\n",
    "\n",
    "samples = eval_dataset.shuffle().select(range(5))\n",
    "\n",
    "obfuscated_samples = model.obfuscate(list(samples[dataset_config.input_column])).obfuscated_images.cpu()\n",
    "obfuscated_images = (obfuscated_samples + 1) / 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(2, len(samples), figsize=(20, 10))\n",
    "\n",
    "for i, (sample, obfuscated_sample) in enumerate(zip(samples, obfuscated_images)):\n",
    "    sample_name = sample[dataset_config.label_column]\n",
    "    sample_image = sample[dataset_config.input_column]\n",
    "\n",
    "    axes[0, i].set_title(f'{sample_name}')\n",
    "    axes[0, i].imshow(sample_image.permute(1, 2, 0))\n",
    "    axes[1, i].imshow(obfuscated_sample.permute(1, 2, 0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba43e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_wo_obfuscation = ZeroShotImageClassificationModel.evaluate_model(model, dataset_config, batch_size=256, with_obfuscation=False)\n",
    "print(f'Evaluation Rsults on Clean Images: accuracy - {eval_result_wo_obfuscation.accuracy}, recall = {eval_result_wo_obfuscation.recall}, precision = {eval_result_wo_obfuscation.precision}, f1 - {eval_result_wo_obfuscation.f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ba630",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_w_obfuscation = ZeroShotImageClassificationModel.evaluate_model(model, dataset_config, batch_size=256, with_obfuscation=True)\n",
    "print(f'Evaluation Rsults on Clean Images: accuracy - {eval_result_w_obfuscation.accuracy}, recall = {eval_result_w_obfuscation.recall}, precision = {eval_result_w_obfuscation.precision}, f1 - {eval_result_w_obfuscation.f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c18752",
   "metadata": {},
   "source": [
    "## Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdabec4",
   "metadata": {},
   "source": [
    "### hustvl/yolos-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c98f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'hustvl/yolos-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed9b9b",
   "metadata": {},
   "source": [
    "#### COCO2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcb64e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff2838789ff4e46aa4a7c20658b3d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9b80c2f4574661b3f4f1b132bda946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from data.model_config import ImageModelConfigWithObfuscation\n",
    "from model.object_detection import ObjectDetectionModel\n",
    "\n",
    "model_config = ImageModelConfigWithObfuscation(\n",
    "    hf_model_name_or_path=MODEL_NAME,\n",
    "    obfuscation_patch_size=16,\n",
    "    obfuscation_group_size=256\n",
    ")\n",
    "\n",
    "model = ObjectDetectionModel(config=model_config, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b71f21fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d6cecf4c2045cbbca352aef3be82ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbd65101d7f45cbbefc7848f7376b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565e7b06ad1a4355b23b4dfc923ea542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training embedding_for_obfuscated_images layer: 100%|██████████| 500/500 [07:58<00:00,  1.05it/s, loss=0.00967, lr=2.41e-7] \n"
     ]
    }
   ],
   "source": [
    "model = ObjectDetectionModel.train_obfuscation_modules(model, iterations=500, learning_rate=1e-2, batch_size=32)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb686826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e218e71f33e340329443af9f259a3f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492fbf23a9204fc08e019eb9116d1875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8bb1bf9fd949bd880387eba6f309d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25df390ef114404faf7219bd5b372872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32d8692bded4de68936fe661c186015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data.dataset_config import load_dataset_config\n",
    "\n",
    "dataset_config = load_dataset_config('coco2017-instances')\n",
    "\n",
    "_, eval_dataset = dataset_config.build()\n",
    "label_names = [name for cls, name in dataset_config.id2label.items()]\n",
    "\n",
    "samples = eval_dataset.shuffle().select(range(5))\n",
    "\n",
    "obfuscated_samples = model.obfuscate(list(samples[dataset_config.input_column])).obfuscated_images.cpu()\n",
    "obfuscated_images = (obfuscated_samples + 1) / 2\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# fig, axes = plt.subplots(2, len(samples), figsize=(20, 10))\n",
    "\n",
    "# for i, (sample, obfuscated_sample) in enumerate(zip(samples, obfuscated_images)):\n",
    "#     sample_name = sample[dataset_config.label_column]\n",
    "#     sample_image = sample[dataset_config.input_column]\n",
    "\n",
    "#     # axes[0, i].set_title(f'{sample_name}')\n",
    "#     axes[0, i].imshow(sample_image)\n",
    "#     axes[1, i].imshow(obfuscated_sample.permute(1, 2, 0))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7196f6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f049f0c0a2d46fc80bbc91a4436cbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01629a8bb8a5458588731d592d6a55b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d7195d800e4533bc6842bf82b7c7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4762c6da1749eb9193317056024f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b24cc2e0e604310aac7edfa01664a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 100%|██████████| 39/39 [12:17<00:00, 18.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Rsults on Clean Images: ObjectDetectionEvaluationOutput(model_outputs=None, map=0.3933364450931549, map_50=0.5836373567581177, map_75=0.41392987966537476, map_small=0.13585682213306427, map_medium=0.36272546648979187, map_large=0.5815038681030273, mar_1=0.32113292813301086, mar_10=0.4898848831653595, mar_100=0.5093196034431458, mar_small=0.21362093091011047, mar_medium=0.4865545630455017, mar_large=0.6877384185791016)\n"
     ]
    }
   ],
   "source": [
    "eval_result_wo_obfuscation = ObjectDetectionModel.evaluate_model(model, dataset_config, batch_size=128, with_obfuscation=False)\n",
    "print(f'Evaluation Rsults on Clean Images: {eval_result_wo_obfuscation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "939f5130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6b95d5111f4f8290ce30a565b9dc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe0b64d38234b11a2ad60296ebb149e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d0ecb652a14c5a9e148f19747b0ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3860ed6944c14d30967bfa13e1a9b116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb295056ab44d6b9e76cdb902f9a05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 100%|██████████| 39/39 [12:15<00:00, 18.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Rsults on Clean Images: ObjectDetectionEvaluationOutput(model_outputs=None, map=0.31676360964775085, map_50=0.4862249195575714, map_75=0.3315560519695282, map_small=0.06555934995412827, map_medium=0.2594332695007324, map_large=0.5187240839004517, mar_1=0.2777983248233795, mar_10=0.4105357527732849, mar_100=0.42434802651405334, mar_small=0.11202989518642426, mar_medium=0.3752598166465759, mar_large=0.6366757750511169)\n"
     ]
    }
   ],
   "source": [
    "eval_result_w_obfuscation = ObjectDetectionModel.evaluate_model(model, dataset_config, batch_size=128, with_obfuscation=True)\n",
    "print(f'Evaluation Rsults on Clean Images: {eval_result_w_obfuscation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a918161",
   "metadata": {},
   "source": [
    "## Multi-Modal Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a809364",
   "metadata": {},
   "source": [
    "### hustvl/yolos-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13394336",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'google/owlvit-base-patch16'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02653c91",
   "metadata": {},
   "source": [
    "#### COCO2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c4d93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360ac16ad16440039e3241517cbd20ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee10d74c6e6d4b7aa71b17021114b53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from data.model_config import ImageModelConfigWithObfuscation\n",
    "from model.object_detection import ZeroShotObjectDetectionModel\n",
    "\n",
    "model_config = ImageModelConfigWithObfuscation(\n",
    "    hf_model_name_or_path=MODEL_NAME,\n",
    "    obfuscation_patch_size=16,\n",
    "    obfuscation_group_size=256\n",
    ")\n",
    "\n",
    "model = ZeroShotObjectDetectionModel(config=model_config, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92538cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d6cecf4c2045cbbca352aef3be82ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbd65101d7f45cbbefc7848f7376b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565e7b06ad1a4355b23b4dfc923ea542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training embedding_for_obfuscated_images layer: 100%|██████████| 500/500 [07:58<00:00,  1.05it/s, loss=0.00967, lr=2.41e-7] \n"
     ]
    }
   ],
   "source": [
    "model = ZeroShotObjectDetectionModel.train_obfuscation_modules(model, iterations=500, learning_rate=1e-2, batch_size=32)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e13d7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624eb8507b52470dab31794aaa234c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d8d32f585d49edb561cc2e7cec3759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a274845a22834ac1b3d528399c84f7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb6cc1a15574b0f975cb352b1a74d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0b827eb565461192f4faece782afc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data.dataset_config import load_dataset_config\n",
    "\n",
    "dataset_config = load_dataset_config('coco2017-instances')\n",
    "\n",
    "_, eval_dataset = dataset_config.build()\n",
    "label_names = [name for cls, name in dataset_config.id2label.items()]\n",
    "\n",
    "samples = eval_dataset.shuffle().select(range(5))\n",
    "\n",
    "obfuscated_samples = model.obfuscate(list(samples[dataset_config.input_column])).obfuscated_images.cpu()\n",
    "obfuscated_images = (obfuscated_samples + 1) / 2\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# fig, axes = plt.subplots(2, len(samples), figsize=(20, 10))\n",
    "\n",
    "# for i, (sample, obfuscated_sample) in enumerate(zip(samples, obfuscated_images)):\n",
    "#     sample_name = sample[dataset_config.label_column]\n",
    "#     sample_image = sample[dataset_config.input_column]\n",
    "\n",
    "#     # axes[0, i].set_title(f'{sample_name}')\n",
    "#     axes[0, i].imshow(sample_image)\n",
    "#     axes[1, i].imshow(obfuscated_sample.permute(1, 2, 0))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "851a697e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b466d85e519548ee8b9c6ce50690d288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597740738bef4e69af939fded35737ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314f94649c5f433a82ff68c2ced67ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64750cfc145147a5a4ee83511af9182a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4aa4dc2e5a4510ab7cc2ecdcdaa8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  11%|█         | 17/155 [00:40<05:31,  2.40s/it]/workspace/.venv/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Encountered more than 100 detections in a single image. This means that certain detections with the lowest scores will be ignored, that may have an undesirable impact on performance. Please consider adjusting the `max_detection_threshold` to suit your use case. To disable this warning, set attribute class `warn_on_many_detections=False`, after initializing the metric.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Evaluating model: 100%|██████████| 155/155 [06:19<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Rsults on Clean Images: ObjectDetectionEvaluationOutput(model_outputs=None, map=0.00023515350767411292, map_50=0.00036320940125733614, map_75=0.00024379484239034355, map_small=0.00047920411452651024, map_medium=0.00034597385092638433, map_large=0.00015085471386555582, mar_1=0.0008346400572918355, mar_10=0.0013814010890200734, mar_100=0.0014160171849653125, mar_small=0.002484560012817383, mar_medium=0.0027587502263486385, mar_large=0.001028957194648683)\n"
     ]
    }
   ],
   "source": [
    "eval_result_wo_obfuscation = ZeroShotObjectDetectionModel.evaluate_model(model, dataset_config, batch_size=32, with_obfuscation=False)\n",
    "print(f'Evaluation Rsults on Clean Images: {eval_result_wo_obfuscation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d14a231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6b95d5111f4f8290ce30a565b9dc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe0b64d38234b11a2ad60296ebb149e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d0ecb652a14c5a9e148f19747b0ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3860ed6944c14d30967bfa13e1a9b116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb295056ab44d6b9e76cdb902f9a05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 100%|██████████| 39/39 [12:15<00:00, 18.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Rsults on Clean Images: ObjectDetectionEvaluationOutput(model_outputs=None, map=0.31676360964775085, map_50=0.4862249195575714, map_75=0.3315560519695282, map_small=0.06555934995412827, map_medium=0.2594332695007324, map_large=0.5187240839004517, mar_1=0.2777983248233795, mar_10=0.4105357527732849, mar_100=0.42434802651405334, mar_small=0.11202989518642426, mar_medium=0.3752598166465759, mar_large=0.6366757750511169)\n"
     ]
    }
   ],
   "source": [
    "eval_result_w_obfuscation = ZeroShotObjectDetectionModel.evaluate_model(model, dataset_config, batch_size=128, with_obfuscation=True)\n",
    "print(f'Evaluation Rsults on Clean Images: {eval_result_w_obfuscation}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
